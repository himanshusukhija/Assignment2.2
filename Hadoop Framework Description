Explain in detail:
● HDFS
● Hadoop cluster
● HDFS blocks
In Big Data distributed file system that provides high-performance access to data access Hadoop clusters.HDFS is a key tool for managing
pools of big data and supporting big data analytics applications.HDFS typically is deployed on low-cost commodity hardware, server
failures are common. The file system is designed to be highly fault-tolerant, however, by facilitating the rapid transfer of data 
between compute nodes and enabling Hadoop systems to continue running if a node fails. That decreases the risk of catastrophic failure,
even in the event that numerous nodes fail.
When HDFS takes in data, it breaks the information down into separate pieces and distributes them to different nodes in a cluster,
allowing for parallel processing. The file system also copies each piece of data multiple times and distributes the copies to individual
nodes, placing at least one copy on a different server rack than the others. As a result, the data on nodes that crash can be found
elsewhere within a cluster, which allows processing to continue while the failure is resolved.
HDFS is built to support applications with large data sets, including individual files that reach into the terabytes. It uses a
master/slave architecture, with each cluster consisting of a single NameNode that manages file system operations and supporting
DataNodes that manage data storage on individual compute nodes.
Goals of HDFS-
Fault detection and recovery : Since HDFS includes a large number of commodity hardware, failure of components is frequent
. Therefore HDFS should have mechanisms for quick and automatic fault detection and recovery.

Huge datasets : HDFS should have hundreds of nodes per cluster to manage the applications having huge datasets.

Hardware at data : A requested task can be done efficiently, when the computation takes place near the data. Especially where huge 
datasets are involved, it reduces the network traffic and increases the throughput

2: Hadoop Cluster

Normally any set of loosely connected or tightly connected computers that work together as a single system is called Cluster.
In simple words, a computer cluster used for Hadoop is called Hadoop Cluster. 
Hadoop cluster is a special type of computational cluster designed for storing and analyzing vast amount of unstructured data 
in a distributed computing environment. These clusters run on low cost commodity computers. 
Hadoop clusters are often referred to as "shared nothing" systems because the only thing that is shared between nodes is 
the network that connects them. 
Large Hadoop Clusters are arranged in several racks. Network traffic between different nodes in the same rack is much more 
desirable than network traffic across the racks. 
Hadoop cluster has 3 components:
1- Client
2- Master
3- Slave
Client: It is neither master nor slave, rather play a role of loading the data into cluster, submit MapReduce jobs describing how 
the data should be processed and then retrieve the data to see the response after job completion. 
Masters:The Masters consists of 3 components NameNode, Secondary Node name and JobTracker. 
1- NameNode
2- JobTracker
3-  Secondary Name Node
Slaves: Slave nodes are the majority of machines in Hadoop Cluster and are responsible to
1- Store the data
2- Process the computation

3: HDFS blocks

When you store a file in HDFS, the system breaks it down into a set of individual blocks and stores these blocks in various slave nodes
in the Hadoop cluster. This is an entirely normal thing to do, as all file systems break files down into blocks before storing them to
disk.
HDFS is often blissfully unaware that the final record in one block may be only a partial record, with the rest of its content shunted
off to the following block. HDFS only wants to make sure that files are split into evenly sized blocks that match the predefined block
size for the Hadoop instance (unless a custom value was entered for the file being stored). In the preceding figure, that block size is
128MB.
The concept of storing a file as a collection of blocks is entirely consistent with how file systems normally work. But what’s 
different about HDFS is the scale. A typical block size that you’d see in a file system under Linux is 4KB, whereas a typical block 
size in Hadoop is 128MB. This value is configurable, and it can be customized, as both a new system default and a custom value for 
individual files.
First of all, every data block stored in HDFS has its own metadata and needs to be tracked by a central server so that applications 
needing to access a specific file can be directed to wherever all the file’s blocks are stored. If the block size were in the kilobyte
range, even modest volumes of data in the terabyte scale would overwhelm the metadata server with too many blocks to track.
Second, HDFS is designed to enable high throughput so that the parallel processing of these large data sets happens as quickly as
possible. The key to Hadoop’s scalability on the data processing side is, and always will be, parallelism — the ability to process 
the individual blocks of these large files in parallel.
